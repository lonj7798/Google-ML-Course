{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfrom Your data\n",
    "- Indroduction to Transforming Data\n",
    "- Transforming Numeric Data\n",
    "- Transforming Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Transforming Data\n",
    "\n",
    "[**Feature engineering**](https://developers.google.com/machine-learning/glossary#feature_engineering) is the process of determining which features might be useful in training a model, and then creating those features by transforming raw data found in log files and other sources. In this section, we focus on when and how to transform numeric and categorical data, and the tradeoffs of different approaches.\n",
    "\n",
    "### Reasons for Data Transformation\n",
    "\n",
    "We transform features primarily for the following reasons:\n",
    "\n",
    "1. **Mandatory transformations** for data compatibility. Examples include:\n",
    "    - Converting non-numeric features into numeric. You canâ€™t do matrix multiplication on a string, so we must convert the string to some numeric representation.\n",
    "    - Resizing inputs to a fixed size. Linear models and feed-forward neural networks have a fixed number of input nodes, so your input data must always have the same size. For example, image models need to reshape the images in their dataset to a fixed size.\n",
    "\n",
    "2. **Optional qualtity transformations** that may help the model perform better. Examples includes:\n",
    "    - Tokenization or lower-casing of text features.\n",
    "    - Normalized numeric features (most models perform better afterwards).\n",
    "    - Allowing linear models to introduce non-linearities into the feature space.\n",
    "    \n",
    "Strictly speaking, quality transformations are not necessary--your model could still run without them But using these techniques may enable the model to give better results.\n",
    "\n",
    "### Where to Transform?\n",
    "\n",
    "You can apply transformations either while generating the data on disk, or within the model.\n",
    "\n",
    "**Transforming prior to training**\n",
    "\n",
    "In this approach, we perform the transformation before training. This code lives separate from your machine learning model.\n",
    "\n",
    "**Pros**\n",
    "- Computation is performed only once.\n",
    "- Computation can look at entire dataset to determine the transformation.\n",
    "\n",
    "**Cons**\n",
    "- Transformations need to be reproduced at prediction time. Beware of skew!\n",
    "- Any transformation changes require rerunning data generation, leading to slower iterations.\n",
    "\n",
    "Skew is more dangerous for cases involving online serving. In offline serving, you might be able to reuse the code that generates your training data. In online serving, the code that creates your dataset and the code used to handle live traffic are almost necessarily different, which makes it easy to introduce skew.\n",
    "\n",
    "**Transforming within the model**\n",
    "\n",
    "For this approach, the transformation is part of the model code. The model takes in untransformed data as input and will transform it within the model.\n",
    "\n",
    "**Pros**\n",
    "- Easy iterations. If you change the transformations, you can still use the same data files.\n",
    "- You're guaranteed the same transformations at training and prediction time.\n",
    "\n",
    "**Cons**\n",
    "- Expensive transforms can increase model latency.\n",
    "- Transformations are per batch.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

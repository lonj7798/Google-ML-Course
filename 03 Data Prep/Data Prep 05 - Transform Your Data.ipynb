{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfrom Your data\n",
    "- Indroduction to Transforming Data\n",
    "- Transforming Numeric Data\n",
    "- Transforming Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Transforming Data\n",
    "\n",
    "[**Feature engineering**](https://developers.google.com/machine-learning/glossary#feature_engineering) is the process of determining which features might be useful in training a model, and then creating those features by transforming raw data found in log files and other sources. In this section, we focus on when and how to transform numeric and categorical data, and the tradeoffs of different approaches.\n",
    "\n",
    "### Reasons for Data Transformation\n",
    "\n",
    "We transform features primarily for the following reasons:\n",
    "\n",
    "1. **Mandatory transformations** for data compatibility. Examples include:\n",
    "    - Converting non-numeric features into numeric. You can’t do matrix multiplication on a string, so we must convert the string to some numeric representation.\n",
    "    - Resizing inputs to a fixed size. Linear models and feed-forward neural networks have a fixed number of input nodes, so your input data must always have the same size. For example, image models need to reshape the images in their dataset to a fixed size.\n",
    "\n",
    "2. **Optional qualtity transformations** that may help the model perform better. Examples includes:\n",
    "    - Tokenization or lower-casing of text features.\n",
    "    - Normalized numeric features (most models perform better afterwards).\n",
    "    - Allowing linear models to introduce non-linearities into the feature space.\n",
    "    \n",
    "Strictly speaking, quality transformations are not necessary--your model could still run without them But using these techniques may enable the model to give better results.\n",
    "\n",
    "### Where to Transform?\n",
    "\n",
    "You can apply transformations either while generating the data on disk, or within the model.\n",
    "\n",
    "**Transforming prior to training**\n",
    "\n",
    "In this approach, we perform the transformation before training. This code lives separate from your machine learning model.\n",
    "\n",
    "**Pros**\n",
    "- Computation is performed only once.\n",
    "- Computation can look at entire dataset to determine the transformation.\n",
    "\n",
    "**Cons**\n",
    "- Transformations need to be reproduced at prediction time. Beware of skew!\n",
    "- Any transformation changes require rerunning data generation, leading to slower iterations.\n",
    "\n",
    "Skew is more dangerous for cases involving online serving. In offline serving, you might be able to reuse the code that generates your training data. In online serving, the code that creates your dataset and the code used to handle live traffic are almost necessarily different, which makes it easy to introduce skew.\n",
    "\n",
    "**Transforming within the model**\n",
    "\n",
    "For this approach, the transformation is part of the model code. The model takes in untransformed data as input and will transform it within the model.\n",
    "\n",
    "**Pros**\n",
    "- Easy iterations. If you change the transformations, you can still use the same data files.\n",
    "- You're guaranteed the same transformations at training and prediction time.\n",
    "\n",
    "**Cons**\n",
    "- Expensive transforms can increase model latency.\n",
    "- Transformations are per batch.\n",
    "\n",
    "There are many considerations for transforming per batch. Suppose you want to [**normalize**](https://developers.google.com/machine-learning/glossary#normalization) a feature by its average value--that is, you want to change the feature values to have mean <code>0</code> and standard deviation <code>1</code>. When transforming inside the model, this normalization will have access to only one batch of data, not the full dataset. You can either normalize by the average value within a batch (dangerous if batches are highly variant), or precompute the average and fix it as a constant in the model. We'll explore normalization in the next section.\n",
    "\n",
    "### Explore, Clean, and Visualize Your Data\n",
    "Explore and clean up your data before performing any transformations on it. You may have done some of the following tasks as you collected and constructed your dataset:\n",
    "\n",
    "- Examine several rows of data.\n",
    "- Check basic statistics.\n",
    "- Fix missing numerical entries.\n",
    "**Visualize your data frequently.** Consider [Anscombe's Quartet](https://en.wikipedia.org/wiki/Anscombe's_quartet): your data can look one way in the basic statistics, and another when graphed. Before you get too far into analysis, look at your data graphically, either via scatter plots or histograms. View graphs not only at the beginning of the pipeline, but also throughout transformation. Visualizations will help you continually check your assumptions and see the effects of any major changes.\n",
    "\n",
    "- [Intro to Pandas (Machine Learning Crash Course Pre-req CoLab)](https://colab.research.google.com/github/google/eng-edu/blob/master/ml/cc/prework/intro_to_pandas.ipynb?utm_source=ss-data-prep&utm_campaign=colab-external&utm_medium=referral&utm_content=pandas-colab)\n",
    "\n",
    "- [Working with Missing Data (Pandas Documentation)](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html)\n",
    "\n",
    "- [Visualizations (Pandas Documentation)](https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html)\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "The goal of normalization is to transform features to be on a similar scale. This improves the performance and training stability of the model.\n",
    "\n",
    "### Normalization Techniques at a Glance\n",
    "\n",
    "Four common normalization techniques may be useful\n",
    "\n",
    "- scaling to range\n",
    "- clipping\n",
    "- log scaling\n",
    "- z-score\n",
    "\n",
    "The following charts show the effect of each normalizaation technique on the distribution of the raw feature (price) on the left. The charts are based on hte data set from 1985 Ward's Automative Yearbook that is part of the [UCI Machine Learning Repository under Automobile Data Set](https://archive.ics.uci.edu/ml/datasets/automobile).\n",
    "\n",
    "![](11.png)\n",
    "\n",
    "#### Scaling to a range\n",
    "\n",
    "[Recall from MLCC](https://developers.google.com/machine-learning/crash-course/representation/cleaning-data) that [**scaling**](https://developers.google.com/machine-learning/glossary#scaling) means converting floating-point feature values from their natural range (for example, 100 to 900) into a standard range—usually 0 and 1 (or sometimes -1 to +1). Use the following simple formula to scale to a range:\n",
    "\n",
    "$$x' = (x-x_\\min) / (x_\\max - x_\\min)$$\n",
    "\n",
    "Scaling to a range is good choice when both of the following conditions are met:\n",
    "\n",
    "- You know the approximate upper and lower bounds on your data with few or no outliers.\n",
    "- Your data is Your data is approximately uniformly distributed across that range.\n",
    "\n",
    "A good example is age. Most age values falls between 0 and 90, and every part of the range has a substantial number of people.\n",
    "\n",
    "In contrast, you would not use scaling on income, because only a few people have very high incomes. The upper bound of the linear scale for income would be very high, and most people would be squeezed into a small part of the scale.\n",
    "\n",
    "#### Feature Clipping\n",
    "If your data set contains extreme outliers, you might try feature clipping, which caps all feature values above (or below) a certain value to fixed value. For example, you could clip all temperature values above 40 to be exactly 40.\n",
    "\n",
    "You may apply feature clipping before or after other normalizations.\n",
    "\n",
    "**Formula: Set min/max values to avoid outliers.**\n",
    "\n",
    "![](12.png)\n",
    "\n",
    "Another simple clipping strategy is to clip by z-score to +-Nσ (for example, limit to +-3σ). Note that σ is the standard deviation.\n",
    "\n",
    "#### Log Scaling\n",
    "Log scaling computes the log of your values to compress a wide range to a narrow range.\n",
    "\n",
    "$$x' = log(x)$$\n",
    "\n",
    "Log scaling is helpful when a handful of your values have many points, while most other values have few points. This data distribution is known as the power law distribution. Movie ratings are a good example. In the chart below, most movies have very few ratings (the data in the tail), while a few have lots of ratings (the data in the head). Log scaling changes the distribution, helping to improve linear model performance.\n",
    "\n",
    "![](13.png)\n",
    "\n",
    "#### Z-Score\n",
    "Z-score is a variation of scaling that represents the number of standard deviations away from the mean. You would use z-score to ensure your feature distributions have mean = 0 and std = 1. It’s useful when there are a few outliers, but not so extreme that you need clipping.\n",
    "\n",
    "The formula for calculating the z-score of a point, x, is as follows:\n",
    "\n",
    "$$x' = (x-\\mu)/\\sigma$$\n",
    "\n",
    "*Note: μ is the mean and σ is the standard deviation.\n",
    "\n",
    "![](14.png)\n",
    "\n",
    "Notice that z-score squeezes raw values that have a range of ~40000 down into a range from roughly -1 to +4.\n",
    "\n",
    "Suppose you're not sure whether the outliers truly are extreme. In this case, start with z-score unless you have feature values that you don't want the model to learn; for example, the values are the result of measurement error or a quirk.\n",
    "\n",
    "### Summary\n",
    "\n",
    "| **Normalization Technique** | **Formula** | **When to Use** |\n",
    "|:------------|:------------|:------------|\n",
    "|Linear Scaling|$x' = (x-x_\\min) / (x_\\max - x_\\min)$|When the feature is more-or-less uniformly distributed across a fixed range.|\n",
    "|Clipping|if x > max, then x' = max. if x < min, then x' = min|When the feature contains some extreme outliers.|\n",
    "|Log Scaling|$x' = log(x)$|When the feature conforms to the power law.|\n",
    "|Z-score|$x' = (x-\\mu)/\\sigma$|\tWhe|n the feature distribution does not contain extreme outliers.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketing\n",
    "Let's start with a quick review of a key idea from Machine Learning Crash Course. Look at the distribution in the chart below.\n",
    "\n",
    "![](15.png)\n",
    "\n",
    "**Q.** Consider Figure 1. If you think latitude might be a good predictor of housing values, should you leave latitude as a floating-point value? \n",
    "\n",
    "    No — there's no linear relationship between latitude and the housing values.\n",
    "    You suspect that individual latitudes and housing values are related, but the relationship is not linear.\n",
    "    \n",
    "In cases like the latitude example, you need to divide the latitudes into buckets to learn something different about housing values for each bucket. This transformation of numeric features into categorical features, using a set of thresholds, is called bucketing (or binning). In this [**bucketing**](https://developers.google.com/machine-learning/glossary#bucketing) example, the boundaries are equally spaced.\n",
    "\n",
    "![](16.png)\n",
    "\n",
    "### Quantile Bucketing\n",
    "Let's revisit our car price dataset with buckets added. With one feature per bucket, the model uses as much capacity for a single example in the >45000 range as for all the examples in the 5000-10000 range. This seems wasteful. How might we improve this situation?\n",
    "\n",
    "![](17.png)\n",
    "\n",
    "The problem is that equally spaced buckets don’t capture this distribution well. The solution lies in creating buckets that each have the same number of points. This technique is called [**quantile bucketing**](https://developers.google.com/machine-learning/glossary#quantile_bucketing). For example, the following figure divides car prices into quantile buckets. In order to get the same number of examples in each bucket, some of the buckets encompass a narrow price span while others encompass a very wide price span.\n",
    "\n",
    "### Bucketing Summary\n",
    "If you choose to bucketize your numerical features, be clear about how you are setting the boundaries and which type of bucketing you’re applying:\n",
    "\n",
    "- **Buckets with equally spaced boundaries**: the boundaries are fixed and encompass the same range (for example, 0-4 degrees, 5-9 degrees, and 10-14 degrees, or {\\\\$5,000-\\\\$9,999, \\\\$10,000-\\\\$14,999, and \\\\$15,000-\\\\$19,999). Some buckets could contain many points, while others could have few or none.\n",
    "\n",
    "- **Buckets with quantile boundaries**: each bucket has the same number of points. The boundaries are not fixed and could encompass a narrow or wide span of values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

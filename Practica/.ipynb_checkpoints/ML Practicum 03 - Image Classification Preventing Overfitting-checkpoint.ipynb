{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preventing Overfitting\n",
    "\n",
    "As with any machine learning model, a key concern when training a convolutional neural network is *overfitting*: a model so tuned to the specifincs of the training data that it is unable to generalize to new examples. Two techniques to prevent overfitting when building a CNN are:\n",
    "\n",
    "- **Data augmentation**: artifically boosting the diversity and number of training examples by performing random transformations to existing images to create a set of new variants (see Figure 7). Data augmentation is especially useful when the original training data set is relatively small.\n",
    "- **Dropout regularization**: Randomly removing units from the neural network during a training gradient step.\n",
    "\n",
    "**For more on dropout regularization, see [Training Neural Nets](https://developers.google.com/machine-learning/crash-course/training-neural-networks/video-lecture) in [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/).\n",
    "\n",
    "![](04.png)\n",
    "\n",
    "**Overfitting is more of a concern when working with smaller training data sets. When working with big data sets (e.g., millions of images), applying dropout is unnecessary, and the value of data augmentation is also diminished.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

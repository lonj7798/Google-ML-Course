{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness: Types of Bias\n",
    "\n",
    "Machine learning models are not inherently objective. Engineers train models by feeding them a data set of training examples, and human involvement in the provision and curation of this data can make a model's predictions susceptible to bias.\n",
    "\n",
    "When building models, it's important to be aware of common human biases that can manifest in your data, so you can take proactive steps to mitigate their effects.\n",
    "\n",
    "\n",
    "**WARNING: The following inventory of biases provides just a small selection of biases that are often uncovered in machine learning data sets; this list is not intended to be exhaustive. Wikipedia's catalog of cognitive biases enumerates over 100 different types of human bias that can affect our judgment. When auditing your data, you should be on the lookout for any and all potential sources of bias that might skew your model's predictions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting Bias\n",
    "**Reporting bias** occurs when the frequency of events, properties, and/or outcomes captured in a data set does not accurately reflect their real-world frequency. This bias can arise because people tend to focus on documenting circumstances that are unusual or especially memorable, assuming that the ordinary can \"go without saying.\"\n",
    "\n",
    "<code>**EXAMPLE**: A sentiment-analysis model is trained to predict whether book reviews are positive or negative based on a corpus of user submissions to a popular website. The majority of reviews in the training data set reflect extreme opinions (reviewers who either loved or hated a book), because people were less likely to submit a review of a book if they did not respond to it strongly. As a result, the model is less able to correctly predict sentiment of reviews that use more subtle language to describe a book.</code>\n",
    "\n",
    "\n",
    "### Automation Bias\n",
    "**Automation bias** is a tendency to favor results generated by automated systems over those generated by non-automated systems, irrespective of the error rates of each.\n",
    "\n",
    "<code>**EXAMPLE**: Software engineers working for a sprocket manufacturer were eager to deploy the new \"groundbreaking\" model they trained to identify tooth defects, until the factory supervisor pointed out that the model's precision and recall rates were both 15% lower than those of human inspectors.</code>\n",
    "\n",
    "\n",
    "### Selection Bias\n",
    "**Selection bias** occurs if a data set's examples are chosen in a way that is not reflective of their real-world distribution. Selection bias can take many different forms:\n",
    "\n",
    "- **Coverage bias**: Data is not selected in a representative fashion.\n",
    "\n",
    "<code>**EXAMPLE**: A model is trained to predict future sales of a new product based on phone surveys conducted with a sample of consumers who bought the product. Consumers who instead opted to buy a competing product were not surveyed, and as a result, this group of people was not represented in the training data.</code>\n",
    "\n",
    "- **Non-response bias** (or **participation bias**): Data ends up being unrepresentative due to participation gaps in the data-collection process.\n",
    "\n",
    "<code>**EXAMPLE**: A model is trained to predict future sales of a new product based on phone surveys conducted with a sample of consumers who bought the product and with a sample of consumers who bought a competing product. Consumers who bought the competing product were 80% more likely to refuse to complete the survey, and their data was underrepresented in the sample.</code>\n",
    "\n",
    "- **Sampling bias**: Proper randomization is not used during data collection.\n",
    "\n",
    "<code>**EXAMPLE**: A model is trained to predict future sales of a new product based on phone surveys conducted with a sample of consumers who bought the product and with a sample of consumers who bought a competing product. Instead of randomly targeting consumers, the surveyer chose the first 200 consumers that responded to an email, who might have been more enthusiastic about the product than average purchasers.</code>\n",
    "\n",
    "### Group Attribution Bias\n",
    "**Group attribution bias** is a tendency to generalize what is true of individuals to an entire group to which they belong. Two key manifestations of this bias are:\n",
    "\n",
    "- **In-group bias**: A preference for members of a group to which you also belong, or for characteristics that you also share.\n",
    "\n",
    "<code>**EXAMPLE**: Two engineers training a résumé-screening model for software developers are predisposed to believe that applicants who attended the same computer-science academy as they both did are more qualified for the role.</code>\n",
    "\n",
    "- **Out-group homogeneity bias**: A tendency to stereotype individual members of a group to which you do not belong, or to see their characteristics as more uniform.\n",
    "\n",
    "<code>**EXAMPLE**: Two engineers training a résumé-screening model for software developers are predisposed to believe that all applicants who did not attend a computer-science academy do not have sufficient expertise for the role.</code>\n",
    "\n",
    "### Implicit Bias\n",
    "**Implicit bias** occurs when assumptions are made based on one's own mental models and personal experiences that do not necessarily apply more generally.\n",
    "\n",
    "<code>**EXAMPLE**: An engineer training a gesture-recognition model uses a [head shake](https://en.wikipedia.org/wiki/Head_shake) as a feature to indicate a person is communicating the word \"no.\" However, in some regions of the world, a head shake actually signifies \"yes.\"</code>\n",
    "\n",
    "A common form of implicit bias is **confirmation bias**, where model builders unconsciously process data in ways that affirm preexisting beliefs and hypotheses. In some cases, a model builder may actually keep training a model until it produces a result that aligns with their original hypothesis; this is called **experimenter's bias**.\n",
    "\n",
    "<code>**EXAMPLE**: An engineer is building a model that predicts aggressiveness in dogs based on a variety of features (height, weight, breed, environment). The engineer had an unpleasant encounter with a hyperactive toy poodle as a child, and ever since has associated the breed with aggression. When the trained model predicted most toy poodles to be relatively docile, the engineer retrained the model several more times until it produced a result showing smaller poodles to be more violent.</code>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
